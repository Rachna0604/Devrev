{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachna0604/Devrev/blob/main/FinalDevrev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeUq8jAymGVD",
        "outputId": "7326f1ea-4568-4e16-cb8c-d99d829214c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nltk==3.4.5) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449924 sha256=f69a1958a8a4ca84643805a23096acddb3acbb5dbbf75bda9e16e88a97f493e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/18/48/8fd6ec11da38406b309470566d6f099c04805d2ec61d7829e7\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed nltk-3.4.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 9.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk==3.4.5\n",
        "!pip install torch\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rekxgx44m_ZF",
        "outputId": "f9882d3e-21d6-41ee-f13f-605aca63358a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "#for pre processing\n",
        "import csv,numpy \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSVzZ4brnEgL",
        "outputId": "2dfc600a-5917-4de1-8d82-b1376d638afb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# accessing g drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQREM4zt3khB"
      },
      "source": [
        "#1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Godp1V5ndOC"
      },
      "source": [
        "Predicting paragraphs from question and theme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxm8h-CHnSx9"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "def predict_answer(question, paragraphs, theme):\n",
        "    # Filter the paragraphs to only include those with the correct theme\n",
        "    theme_paragraphs = [p['text'] for p in paragraphs if p['theme'] == theme]\n",
        "    \n",
        "    # Create a TfidfVectorizer to convert the paragraphs into numerical vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(theme_paragraphs)\n",
        "    \n",
        "    # Convert the question into a numerical vector\n",
        "    question_vector = vectorizer.transform([question])\n",
        "    \n",
        "    # Calculate the cosine similarity between the question and each paragraph\n",
        "    similarity = cosine_similarity(question_vector, X)\n",
        "    \n",
        "    # Find the paragraph with the highest similarity score\n",
        "    max_similarity_index = similarity.argmax()\n",
        "    max_similarity_score = similarity[0, max_similarity_index]\n",
        "    \n",
        "    # If the maximum similarity score is above a certain threshold, return the paragraph\n",
        "    if max_similarity_score > 0.05:\n",
        "        return theme_paragraphs[max_similarity_index]\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zE99rQOnq8B",
        "outputId": "b09b7157-c094-4beb-c73d-99078e091633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paris is the capital of France\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the capital of France?\"\n",
        "paragraphs = [\n",
        "    {'text': 'Paris is the capital of France', 'theme': 'Geography'},\n",
        "    {'text': 'Lyon is a city in France', 'theme': 'Geography'},\n",
        "    {'text': 'The Pythagorean theorem states that a^2 + b^2 = c^2', 'theme': 'Mathematics'},\n",
        "]\n",
        "\n",
        "# Predict the answer to the question\n",
        "answer = predict_answer(question, paragraphs, 'Geography')\n",
        "\n",
        "# Print the answer\n",
        "if answer:\n",
        "    print(answer)\n",
        "else:\n",
        "    print(\"No suitable paragraph found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOZVm7YIn1x4"
      },
      "source": [
        "Saving the paragraphs,questions and themes in the way it can be passed through the function predict_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPmZDuC8nuvk",
        "outputId": "3fc762ae-4b8a-4d41-fe57-ae11a22ac91a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/DevRev/train_data.csv'\n",
        "data = pd.read_csv(path)\n",
        "Paragraphs = []\n",
        "for para in data['Paragraph']:\n",
        "    if para not in Paragraphs:\n",
        "        Paragraphs.append(para)\n",
        "questions = []\n",
        "for q in data['Question']:\n",
        "    if q not in questions:\n",
        "          questions.append(q)\n",
        "themes = []\n",
        "for t in data['Theme']:\n",
        "    if t not in themes:\n",
        "        themes.append(t)\n",
        "PT = []\n",
        "Themes = []\n",
        "for element in Paragraphs:\n",
        "    Themes.append(data['Theme'][data.loc[data['Paragraph'] == element]['Theme'].index[0]])\n",
        "for i in range(len(Themes)):\n",
        "    PT.append({'text': Paragraphs[i], 'theme': Themes[i]})\n",
        "question = questions[0]\n",
        "answer = predict_answer(question, PT, 'Beyoncé')\n",
        "if answer:\n",
        "    print(answer)\n",
        "else:\n",
        "    print(\"No suitable paragraph found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhB46MQO3sBa"
      },
      "source": [
        "#2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teSM1N1Fo1H7"
      },
      "source": [
        "Traning a model    \n",
        "**this has no continuation with the above code , it's a seperate method tried "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ho3SAOSYpPgj",
        "outputId": "b6464363-7491-47f6-aa6f-d2e92cf5f861"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.622876557191393\n",
            "Precision: 0.7600728344154949\n",
            "Recall: 0.5839213487139984\n",
            "F1 Score: 0.6357891846760749\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DevRev/train_data.csv\")\n",
        "\n",
        "# Preprocess the data\n",
        "df[\"question\"] = df[\"Question\"].str.lower()\n",
        "df[\"paragraph\"] = df[\"Paragraph\"].str.lower()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[[\"question\", \"paragraph\"]], df[\"Theme\"], test_size=0.2)\n",
        "\n",
        "# Vectorize the data using a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectors = vectorizer.fit_transform(X_train[\"question\"])\n",
        "X_test_vectors = vectorizer.transform(X_test[\"question\"])\n",
        "\n",
        "# Train a support vector machine (SVM) classifier\n",
        "clf = SVC()\n",
        "clf.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Predict the theme labels for the test data\n",
        "y_pred = clf.predict(X_test_vectors)\n",
        "\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate the precision of the model\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Calculate the recall of the model\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Calculate the F1 score of the model\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MKlguCijpkZv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "1f7a4cf7-e986-4d85-a027-9b84e6550781"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2dd3675d0558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Vectorize the new question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnew_question_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_question\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Use the model to make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Preprocess the new question\n",
        "new_question = \"Who was Hayek's father?\"\n",
        "new_question = new_question.lower()\n",
        "\n",
        "# Vectorize the new question\n",
        "new_question_vector = vectorizer.transform([new_question])\n",
        "\n",
        "# Use the model to make a prediction\n",
        "prediction = clf.predict(new_question_vector)[0]\n",
        "\n",
        "# Print the prediction\n",
        "print(\"Prediction:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th08gOx53v4J"
      },
      "source": [
        "#3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB5sM1QRqISS"
      },
      "source": [
        "Below is another method of approaching the question which I tried earlier, it's not completely working, but approach can be referred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsg0glXnqXWt"
      },
      "outputs": [],
      "source": [
        "def predicting_answer(question, paragraphs):\n",
        "  #pre process\n",
        "  question_tokens = preprocess_text(question)\n",
        "  paragraphs_tokens = [preprocess_text(p) for p in paragraphs]\n",
        "  #generate embeddings\n",
        "  question_embeddings = nlp_model(question_tokens)\n",
        "  paragraphs_embeddings = [nlp_model(p) for p in paragraphs_tokens]\n",
        "  #compare embeddings i.e. similarity measure\n",
        "  most_similar_paragraph = find_most_similar(question_embeddings, paragraphs_embeddings)\n",
        "  #threshold\n",
        "  if most_similar_paragraph[1] > SIMILARITY_THRESHOLD:\n",
        "    return paragraphs[most_similar_paragraph[0]]\n",
        "  else:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av2u0WARIpKo"
      },
      "source": [
        "Pre processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w5wkFtmIqxO",
        "outputId": "ea68f80d-fe9c-4c13-e2fc-9466cdb9478d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['purpos', 'preprocess', 'text', 'data']\n"
          ]
        }
      ],
      "source": [
        "# column index of the column we want to read\n",
        "column_index = 2\n",
        "\n",
        "# list to store the column values\n",
        "column_values = []\n",
        "\n",
        "# open the CSV file in read mode\n",
        "'''\n",
        "with open('/content/drive/MyDrive/DevRev/train_data.csv', 'r') as csv_file:\n",
        "    # creating a CSV reader object\n",
        "    reader = csv.reader(csv_file)\n",
        "    \n",
        "    # iterating over the rows in the CSV file\n",
        "    for row in reader:\n",
        "        # appending the value at the specified column index to the list\n",
        "        column_values.append(row[column_index])\n",
        "'''\n",
        "path = '/content/drive/MyDrive/DevRev/train_data.csv'\n",
        "data = pd.read_csv(path)\n",
        "for para in data['Paragraph']:\n",
        "  column_values.append(para)\n",
        "\n",
        "# column_values now contains the values from the specified column\n",
        "\n",
        "# list of paragraphs\n",
        "paragraphs = column_values\n",
        "\n",
        "# question\n",
        "question = \"What is the purpose of preprocessing text data?\"\n",
        "\n",
        "# lowercase the paragraphs and question\n",
        "paragraphs = [p.lower() for p in paragraphs]\n",
        "question = question.lower()\n",
        "\n",
        "# removing stop words and punctuation from the paragraphs and question\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(['.', ',', ':', ';', '?'])\n",
        "\n",
        "filtered_paragraphs = []\n",
        "for p in paragraphs:\n",
        "    filtered_p = []\n",
        "    for w in word_tokenize(p):\n",
        "        if w not in stop_words and w not in punctuation:\n",
        "            filtered_p.append(w)\n",
        "    filtered_paragraphs.append(filtered_p)\n",
        "\n",
        "filtered_question = []\n",
        "for w in word_tokenize(question):\n",
        "    if w not in stop_words and w not in punctuation:\n",
        "        filtered_question.append(w)\n",
        "\n",
        "# stem the paragraphs and question\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_paragraphs = []\n",
        "for p in filtered_paragraphs:\n",
        "    stemmed_p = []\n",
        "    for w in p:\n",
        "        stemmed_p.append(stemmer.stem(w))\n",
        "    stemmed_paragraphs.append(stemmed_p)\n",
        "\n",
        "stemmed_question = []\n",
        "for w in filtered_question:\n",
        "    stemmed_question.append(stemmer.stem(w))\n",
        "\n",
        "# create a list of tokens for the paragraphs and question\n",
        "tokens_p = []\n",
        "tokens_q = []\n",
        "for p in stemmed_paragraphs:\n",
        "    tokens_p.append(p)\n",
        "tokens_q.extend(stemmed_question)\n",
        "print(tokens_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCjMDtRrqyyo",
        "outputId": "77ce38a6-3cf5-4e26-bf5e-c0dcf31b2f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['what', 'is', 'the', 'capit', 'of', 'franc', '?']\n",
            "['pari', 'is', 'the', 'capit', 'of', 'franc', '.', 'It', 'is', 'locat', 'in', 'the', 'northern', 'central', 'part', 'of', 'the', 'countri', 'and', 'is', 'home', 'to', 'mani', 'famou', 'landmark', ',', 'includ', 'the', 'eiffel', 'tower', '.']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "  # Tokenize the text\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  \n",
        "  # Stem the tokens\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "  \n",
        "  return stemmed_tokens\n",
        "\n",
        "# Pre-process the question\n",
        "question = \"What is the capital of France?\"\n",
        "question_tokens = preprocess_text(question)\n",
        "print(question_tokens)  \n",
        "\n",
        "# Pre-process a paragraph\n",
        "paragraph = \"Paris is the capital of France. It is located in the northern central part of the country and is home to many famous landmarks, including the Eiffel Tower.\"\n",
        "paragraph_tokens = preprocess_text(paragraph)\n",
        "print(paragraph_tokens)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaiBclf4q4AZ",
        "outputId": "0058f392-98f6-4c76-c0cf-40994591642f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  2054,   102,     0],\n",
            "        [  101,  2003,   102,     0],\n",
            "        [  101,  1996,   102,     0],\n",
            "        [  101,  6178,  4183,   102],\n",
            "        [  101,  1997,   102,     0],\n",
            "        [  101, 23151,  2278,   102],\n",
            "        [  101,  1029,   102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0],\n",
            "        [1, 1, 1, 0],\n",
            "        [1, 1, 1, 0],\n",
            "        [1, 1, 1, 1],\n",
            "        [1, 1, 1, 0],\n",
            "        [1, 1, 1, 1],\n",
            "        [1, 1, 1, 0]])}\n",
            "{'input_ids': tensor([[  101, 11968,  2072,   102,     0],\n",
            "        [  101,  2003,   102,     0,     0],\n",
            "        [  101,  1996,   102,     0,     0],\n",
            "        [  101,  6178,  4183,   102,     0],\n",
            "        [  101,  1997,   102,     0,     0],\n",
            "        [  101, 23151,  2278,   102,     0],\n",
            "        [  101,  1012,   102,     0,     0],\n",
            "        [  101,  2009,   102,     0,     0],\n",
            "        [  101,  2003,   102,     0,     0],\n",
            "        [  101,  8840, 11266,   102,     0],\n",
            "        [  101,  1999,   102,     0,     0],\n",
            "        [  101,  1996,   102,     0,     0],\n",
            "        [  101,  2642,   102,     0,     0],\n",
            "        [  101,  2430,   102,     0,     0],\n",
            "        [  101,  2112,   102,     0,     0],\n",
            "        [  101,  1997,   102,     0,     0],\n",
            "        [  101,  1996,   102,     0,     0],\n",
            "        [  101,  4175,  3089,   102,     0],\n",
            "        [  101,  1998,   102,     0,     0],\n",
            "        [  101,  2003,   102,     0,     0],\n",
            "        [  101,  2188,   102,     0,     0],\n",
            "        [  101,  2000,   102,     0,     0],\n",
            "        [  101, 23624,   102,     0,     0],\n",
            "        [  101,  6904,  5302,  2226,   102],\n",
            "        [  101,  8637,   102,     0,     0],\n",
            "        [  101,  1010,   102,     0,     0],\n",
            "        [  101,  4297,  7630,  2094,   102],\n",
            "        [  101,  1996,   102,     0,     0],\n",
            "        [  101,  1041, 13355,  2884,   102],\n",
            "        [  101,  3578,   102,     0,     0],\n",
            "        [  101,  1012,   102,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "# Initialize the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def generate_embeddings(tokens):\n",
        "  # Encode the text\n",
        "  input_ids = tokenizer(tokens, return_tensors='pt',padding=True,truncation=True)\n",
        "  #the output is input tensors in the form of a PyTorch tensor\n",
        "  print(input_ids)\n",
        "# Generate embeddings for the question tokens\n",
        "question_embeddings = generate_embeddings(question_tokens)\n",
        "\n",
        "# Generate embeddings for the paragraph tokens\n",
        "paragraph_embeddings = generate_embeddings(paragraph_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCEO_sCjrErQ"
      },
      "outputs": [],
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def cosine_similarity(emb1, emb2):\n",
        "    similarity = 1 - spatial.distance.cosine(emb1, emb2)\n",
        "    return similarity\n",
        "\n",
        "#note\n",
        "#in this we have to give the axes so for each paragraph in the func defined\n",
        "#so the word embeddings should be stored in a list, that list hould be passed the function\n",
        "#like below given hashtags\n",
        "#paragraph_embeddings = [word_embedding_1, word_embedding_2, word_embedding_3]\n",
        "#question_embeddings = [word_embedding_4, word_embedding_5, word_embedding_6]\n",
        "\n",
        "# Calculating the cosine similarity between the two paragraphs\n",
        "similarity = cosine_similarity(paragraph_embeddings, question_embeddings)\n",
        "\n",
        "# Determining which paragraph is more similar based on the similarity value\n",
        "if similarity > 0.8:\n",
        "    print(paragraph)\n",
        "else:\n",
        "    print(\"Paragraph is not similar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFwK5oefrIxe"
      },
      "outputs": [],
      "source": [
        "#note paragraphs embeddings means there are a lot of paragraphs' embeddings *****\n",
        "def find_most_similar(question_embeddings, paragraphs_embeddings):\n",
        "  max_similarity = -1\n",
        "  most_similar_paragraph = None\n",
        "  for i, paragraph_embedding in enumerate(paragraphs_embeddings):\n",
        "    similarity = cosine_similarity(question_embeddings, paragraph_embedding)\n",
        "    if similarity > max_similarity:\n",
        "      max_similarity = similarity\n",
        "      most_similar_paragraph = i\n",
        "  return most_similar_paragraph, max_similarity"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIZJSmdiaEJNVgNekPuLvH",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}